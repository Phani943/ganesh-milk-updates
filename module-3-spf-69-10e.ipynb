{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5787457,"sourceType":"datasetVersion","datasetId":3324119},{"sourceId":12527753,"sourceType":"datasetVersion","datasetId":7908239}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Subset, Dataset, DataLoader, random_split\nfrom transformers import BertTokenizer, BertModel, ViTModel, ViTImageProcessor","metadata":{"_uuid":"3130e395-117e-4cbb-9dfc-a545ddd3e1ce","_cell_guid":"5da473d8-fe21-42e6-8a63-39d573c1eb59","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:19:13.992443Z","iopub.execute_input":"2025-11-12T04:19:13.992736Z","iopub.status.idle":"2025-11-12T04:19:24.244175Z","shell.execute_reply.started":"2025-11-12T04:19:13.992710Z","shell.execute_reply":"2025-11-12T04:19:24.243543Z"}},"outputs":[{"name":"stderr","text":"2025-11-12 04:19:19.594955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762921159.617362     137 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762921159.624317     137 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# =============================================================================\n# Dataset\n# =============================================================================\nclass FashionGenDataset(Dataset):\n    def __init__(self, h5_path, visualize=False):\n        self.h5 = h5py.File(h5_path, 'r')\n        self.texts = self.h5['input_description']\n        self.imgs = self.h5['input_image']\n        self.visualize = visualize\n        self.fe = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        caption = self.texts[idx][0].decode('utf-8', errors='ignore')\n        raw_img = Image.fromarray(self.imgs[idx])\n        cropped_img = self.crop_and_resize_img(raw_img)\n        pixel_values = self.fe(images=cropped_img, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n        return caption, pixel_values\n\n    @staticmethod\n    def crop_and_resize_img(img, threshold=245):\n        img_np = np.array(img)\n        non_white_mask = np.any(img_np < threshold, axis=2)\n        if not np.any(non_white_mask):\n            return img.resize((224, 224))\n        coords = np.argwhere(non_white_mask)\n        y0, x0 = coords.min(axis=0)\n        y1, x1 = coords.max(axis=0) + 1\n        cropped_img = img.crop((x0, y0, x1, y1))\n        resized_img = cropped_img.resize((224, 224))\n        return resized_img","metadata":{"_uuid":"99e29652-ff6f-45ad-a9ea-9aca0897ab54","_cell_guid":"fdc71c38-bb56-4754-abd9-3fc3b8aaa7fa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:19:24.244922Z","iopub.execute_input":"2025-11-12T04:19:24.245417Z","iopub.status.idle":"2025-11-12T04:19:24.252483Z","shell.execute_reply.started":"2025-11-12T04:19:24.245397Z","shell.execute_reply":"2025-11-12T04:19:24.251961Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# Encoders\n# =============================================================================\nclass TextEncoder(nn.Module):\n    def __init__(self, concept_terms, proj_dim=500, sim_threshold=0.7, freeze_bert=False):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.proj = nn.Linear(768, proj_dim)\n        self.threshold = sim_threshold\n        self.concept_terms = concept_terms\n        self.freeze_bert = freeze_bert\n\n        with torch.no_grad():\n            concept_tokens = self.tokenizer(concept_terms, return_tensors=\"pt\", padding=True, truncation=True)\n            concepts_output = self.bert(**concept_tokens).last_hidden_state\n            concepts_mask = concept_tokens[\"attention_mask\"].unsqueeze(-1)\n            concepts_avg = (concepts_output * concepts_mask).sum(dim=1) / concepts_mask.sum(dim=1)\n        self.register_buffer(\"concept_raw_embeds\", concepts_avg)\n\n    def forward(self, texts):\n        device = next(self.parameters()).device\n        tokens = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n        attn_mask = tokens[\"attention_mask\"].unsqueeze(-1)\n\n        if self.freeze_bert:\n            with torch.no_grad():\n                output = self.bert(**tokens).last_hidden_state\n        else:\n            output = self.bert(**tokens).last_hidden_state\n\n        output_norm = F.normalize(output, dim=-1)\n        concept_norm = F.normalize(self.concept_raw_embeds, dim=-1)\n        sim = torch.matmul(output_norm, concept_norm.T)\n\n        enriched = []\n        for b in range(sim.size(0)):\n            enriched_words = []\n            for l in range(sim.size(1)):\n                if attn_mask[b, l] == 0:\n                    continue\n                sims = sim[b, l]\n                valid_mask = sims >= self.threshold\n                if valid_mask.any():\n                    max_idx = sims[valid_mask].argmax()\n                    concept_idx = valid_mask.nonzero(as_tuple=False)[max_idx]\n                    best_concept = self.concept_raw_embeds[concept_idx]\n                    enriched_word = (output[b, l] + best_concept) / 2\n                else:\n                    enriched_word = output[b, l]\n                enriched_words.append(enriched_word)\n            enriched_sentence = torch.stack(enriched_words).mean(dim=0)\n            enriched.append(enriched_sentence)\n\n        enriched = torch.stack(enriched)\n        return self.proj(enriched)\n\n\nclass ImageEncoder(nn.Module):\n    def __init__(self, proj_dim=500, topk=8):\n        super().__init__()\n        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224', output_attentions=True)\n        self.proj = nn.Linear(768, proj_dim)\n        self.topk = topk\n\n    def forward(self, pixel_values):\n        vit_out = self.vit(pixel_values=pixel_values)\n        out = vit_out.last_hidden_state\n        attn = vit_out.attentions[-1]\n        patches = out[:, 1:]\n        scores = attn[:, :, 0, 1:].mean(dim=1)\n        k = min(self.topk, patches.size(1))\n        idx = scores.topk(k, dim=1).indices\n        selected = patches.gather(1, idx.unsqueeze(-1).expand(-1, -1, patches.size(2)))\n        avg = selected.mean(dim=1)\n        return self.proj(avg)","metadata":{"_uuid":"8b2f12a7-5a95-4aac-bc7e-482838ecf329","_cell_guid":"93fe00b3-f82e-44ca-bf88-282eeaa9dc49","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:19:24.253277Z","iopub.execute_input":"2025-11-12T04:19:24.253515Z","iopub.status.idle":"2025-11-12T04:19:24.270456Z","shell.execute_reply.started":"2025-11-12T04:19:24.253493Z","shell.execute_reply":"2025-11-12T04:19:24.269801Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# =============================================================================\n# SPF Module (Corrected)\n# =============================================================================\nclass SemanticProgressiveFusionModule(nn.Module):\n    def __init__(self, hidden_dim=500, num_prototypes=32, tau1=0.07, tau2=0.1, \n                 eta=1.0, gamma1=0.5, beta1=0.5):\n        super().__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.num_prototypes = num_prototypes\n        \n        self.tau1 = nn.Parameter(torch.tensor(tau1))\n        self.tau2 = nn.Parameter(torch.tensor(tau2))\n        self.eta = nn.Parameter(torch.tensor(eta))\n        self.gamma1 = nn.Parameter(torch.tensor(gamma1))\n        self.beta1 = nn.Parameter(torch.tensor(beta1))\n        \n        # Stage 1\n        self.W_text_1 = nn.Linear(hidden_dim, hidden_dim)\n        self.W_image_1 = nn.Linear(hidden_dim, hidden_dim)\n        \n        # Stage 2\n        self.prototypes = nn.Parameter(torch.randn(num_prototypes, hidden_dim))\n        nn.init.xavier_uniform_(self.prototypes)\n        \n        self.W_image_2 = nn.Linear(hidden_dim, hidden_dim)\n        self.W_text_2 = nn.Linear(hidden_dim, hidden_dim)\n        \n        # Stage 3\n        self.text_out_proj = nn.Linear(hidden_dim, hidden_dim)\n        self.image_out_proj = nn.Linear(hidden_dim, hidden_dim)\n        \n        self.norm_text = nn.LayerNorm(hidden_dim)\n        self.norm_image = nn.LayerNorm(hidden_dim)\n        self.dropout = nn.Dropout(0.1)\n        \n    def compute_entropy(self, attention_weights):\n        eps = 1e-8\n        attention_weights = attention_weights.clamp(min=eps)\n        entropy = -torch.sum(attention_weights * torch.log(attention_weights + eps), dim=-1)\n        return entropy\n    \n    def stage1_fine_grained_alignment(self, X_text, X_image):\n        text_proj = self.W_text_1(X_text)\n        image_proj = self.W_image_1(X_image)\n        \n        S_1 = torch.tanh(torch.matmul(text_proj.unsqueeze(1), image_proj.unsqueeze(2)).squeeze())\n        A_1 = F.softmax(S_1 / self.tau1, dim=0)\n        X_text_1 = X_text + self.gamma1 * A_1.unsqueeze(-1) * X_image\n        entropy_1 = self.compute_entropy(A_1.unsqueeze(-1))\n        \n        return X_text_1, entropy_1, A_1\n    \n    def stage2_part_level_prototypes(self, X_text_1, X_image, A_1):\n        P_2_norm = F.normalize(self.prototypes, dim=-1)\n        image_proj = self.W_image_2(X_image)\n        image_proj_norm = F.normalize(image_proj, dim=-1)\n        \n        proto_sim = torch.matmul(image_proj_norm, P_2_norm.t())\n        R_2 = F.softmax(proto_sim / self.tau2, dim=-1)\n        U_2 = torch.matmul(R_2, self.prototypes)\n        \n        text_proj = self.W_text_2(X_text_1)\n        part_affinity = torch.matmul(text_proj.unsqueeze(1), U_2.unsqueeze(2)).squeeze()\n        \n        # ✅ CRITICAL: beta1 residual\n        A_2 = torch.sigmoid(part_affinity) + self.beta1 * A_1\n        X_text_2 = A_2.unsqueeze(-1) * U_2\n        entropy_2 = self.compute_entropy(R_2)\n        \n        return X_text_2, X_image, entropy_2, R_2\n    \n    def stage3_entropy_regulated_fusion(self, X_text, X_image, \n                                       X_text_1, entropy_1,\n                                       X_text_2, X_image_2, entropy_2):\n        entropy_0 = torch.zeros_like(entropy_1)\n        \n        conf_0 = torch.exp(-entropy_0 / self.eta)\n        conf_1 = torch.exp(-entropy_1 / self.eta)\n        conf_2 = torch.exp(-entropy_2 / self.eta)\n        \n        total_conf = conf_0 + conf_1 + conf_2 + 1e-8\n        \n        omega_0 = conf_0 / total_conf\n        omega_1 = conf_1 / total_conf\n        omega_2 = conf_2 / total_conf\n        \n        X_text_fused = (omega_0.unsqueeze(-1) * X_text + \n                        omega_1.unsqueeze(-1) * X_text_1 + \n                        omega_2.unsqueeze(-1) * X_text_2)\n        \n        X_image_fused = (omega_0.unsqueeze(-1) * X_image + \n                         omega_1.unsqueeze(-1) * X_image + \n                         omega_2.unsqueeze(-1) * X_image_2)\n        \n        X_text_final = self.text_out_proj(X_text_fused)\n        X_image_final = self.image_out_proj(X_image_fused)\n        \n        X_text_final = self.norm_text(X_text_final)\n        X_image_final = self.norm_image(X_image_final)\n        \n        X_text_final = self.dropout(X_text_final)\n        X_image_final = self.dropout(X_image_final)\n        \n        stats = {\n            'omega_0': omega_0.mean().item(),\n            'omega_1': omega_1.mean().item(),\n            'omega_2': omega_2.mean().item(),\n            'entropy_1': entropy_1.mean().item(),\n            'entropy_2': entropy_2.mean().item()\n        }\n        \n        return X_text_final, X_image_final, stats\n    \n    def forward(self, X_text, X_image):\n        X_text_1, entropy_1, A_1 = self.stage1_fine_grained_alignment(X_text, X_image)\n        X_text_2, X_image_2, entropy_2, R_2 = self.stage2_part_level_prototypes(\n            X_text_1, X_image, A_1\n        )\n        X_text_final, X_image_final, stats = self.stage3_entropy_regulated_fusion(\n            X_text, X_image, X_text_1, entropy_1, X_text_2, X_image_2, entropy_2\n        )\n        \n        return X_text_final, X_image_final, stats","metadata":{"_uuid":"79c23e0a-c680-4899-a61a-c20fcd35c62c","_cell_guid":"a3f4addd-c478-4ef4-ad4a-22fa58336508","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:19:24.272098Z","iopub.execute_input":"2025-11-12T04:19:24.272332Z","iopub.status.idle":"2025-11-12T04:19:24.289642Z","shell.execute_reply.started":"2025-11-12T04:19:24.272317Z","shell.execute_reply":"2025-11-12T04:19:24.289092Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# =============================================================================\n# Training Functions\n# =============================================================================\ndef train_spf_module(text_encoder, image_encoder, train_loader, val_loader, \n                     epochs=10, lr=1e-4, device='cuda'):\n    spf = SemanticProgressiveFusionModule(\n        hidden_dim=500, num_prototypes=32, tau1=0.07, tau2=0.1,\n        eta=1.0, gamma1=0.5, beta1=0.5\n    ).to(device)\n    \n    optimizer = torch.optim.AdamW(spf.parameters(), lr=lr, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    text_encoder.eval()\n    image_encoder.eval()\n    spf.train()\n    \n    print(f\"Training SPF: {sum(p.numel() for p in spf.parameters()):,} params | LR: {lr} | Epochs: {epochs}\\n\")\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(epochs):\n        epoch_loss = 0.0\n        epoch_align = 0.0\n        num_batches = 0\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for captions, images in progress_bar:\n            images = images.to(device)\n            batch_size = images.size(0)\n            \n            with torch.no_grad():\n                text_feats = text_encoder(captions)\n                image_feats = image_encoder(images)\n            \n            refined_text, refined_image, stats = spf(text_feats, image_feats)\n            \n            # Contrastive loss\n            text_norm = F.normalize(refined_text, dim=-1)\n            image_norm = F.normalize(refined_image, dim=-1)\n            logits = torch.matmul(text_norm, image_norm.t()) / 0.07\n            labels = torch.arange(batch_size, device=device)\n            \n            loss_t2i = F.cross_entropy(logits, labels)\n            loss_i2t = F.cross_entropy(logits.t(), labels)\n            contrastive_loss = (loss_t2i + loss_i2t) / 2\n            \n            # Alignment loss\n            positive_sim = F.cosine_similarity(refined_text, refined_image, dim=1)\n            alignment_loss = -positive_sim.mean()\n            \n            # Entropy regularization\n            entropy_loss = stats['entropy_1'] + stats['entropy_2']\n            \n            # Diversity regularization\n            proto_sim = torch.matmul(\n                F.normalize(spf.prototypes, dim=-1),\n                F.normalize(spf.prototypes, dim=-1).t()\n            )\n            proto_sim = proto_sim - torch.eye(spf.num_prototypes, device=device)\n            diversity_loss = torch.abs(proto_sim).mean()\n            \n            total_loss = (\n                1.0 * contrastive_loss +\n                0.3 * alignment_loss +\n                0.1 * entropy_loss +\n                0.1 * diversity_loss\n            )\n            \n            optimizer.zero_grad()\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(spf.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            epoch_loss += total_loss.item()\n            epoch_align += positive_sim.mean().item()\n            num_batches += 1\n            \n            progress_bar.set_postfix({\n                'loss': f\"{total_loss.item():.4f}\",\n                'align': f\"{positive_sim.mean().item():.4f}\",\n                'ω1': f\"{stats['omega_1']:.3f}\",\n                'ω2': f\"{stats['omega_2']:.3f}\"\n            })\n        \n        avg_loss = epoch_loss / num_batches\n        avg_align = epoch_align / num_batches\n        \n        print(f\"\\nEpoch {epoch+1}/{epochs}: Loss={avg_loss:.4f}, Align={avg_align:.4f}\")\n        \n        scheduler.step()\n        \n        if val_loader is not None:\n            val_loss, val_align = validate_spf(spf, text_encoder, image_encoder, val_loader, device)\n            print(f\"Val: Loss={val_loss:.4f}, Align={val_align:.4f}\")\n            \n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save({\n                    'epoch': epoch + 1,\n                    'model_state_dict': spf.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'val_loss': val_loss,\n                    'config': {\n                        'hidden_dim': 500,\n                        'num_prototypes': 32,\n                        'tau1': spf.tau1.item(),\n                        'tau2': spf.tau2.item(),\n                        'eta': spf.eta.item(),\n                        'gamma1': spf.gamma1.item(),\n                        'beta1': spf.beta1.item()\n                    }\n                }, 'spf_best.pth')\n                print(\"✅ Best model saved!\")\n        print()\n    \n    return spf\n\ndef validate_spf(spf, text_encoder, image_encoder, val_loader, device):\n    spf.eval()\n    total_loss = 0.0\n    total_align = 0.0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for captions, images in val_loader:\n            images = images.to(device)\n            batch_size = images.size(0)\n            \n            text_feats = text_encoder(captions)\n            image_feats = image_encoder(images)\n            refined_text, refined_image, _ = spf(text_feats, image_feats)\n            \n            text_norm = F.normalize(refined_text, dim=-1)\n            image_norm = F.normalize(refined_image, dim=-1)\n            logits = torch.matmul(text_norm, image_norm.t()) / 0.07\n            labels = torch.arange(batch_size, device=device)\n            \n            loss_t2i = F.cross_entropy(logits, labels)\n            loss_i2t = F.cross_entropy(logits.t(), labels)\n            loss = (loss_t2i + loss_i2t) / 2\n            \n            alignment = F.cosine_similarity(refined_text, refined_image, dim=1).mean()\n            \n            total_loss += loss.item()\n            total_align += alignment.item()\n            num_batches += 1\n    \n    spf.train()\n    return total_loss / num_batches, total_align / num_batches","metadata":{"_uuid":"eceff30b-3e19-4118-9c31-11de79a873e3","_cell_guid":"443e43fa-0714-4412-83a9-d633f9f74372","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:19:24.290345Z","iopub.execute_input":"2025-11-12T04:19:24.290533Z","iopub.status.idle":"2025-11-12T04:19:24.307880Z","shell.execute_reply.started":"2025-11-12T04:19:24.290517Z","shell.execute_reply":"2025-11-12T04:19:24.307268Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# =============================================================================\n# Main Training\n# =============================================================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\\n\")\n\nprint(\"Loading dataset...\")\nfull_ds = FashionGenDataset('/kaggle/input/fashiongen-validation/fashiongen_256_256_train.h5')\n\nsubset_indices = list(range(60000, 90000))\nsubset_ds = Subset(full_ds, subset_indices)\n\ntrain_size = int(0.9 * len(subset_ds))\nval_size = len(subset_ds) - train_size\ntrain_ds, val_ds = random_split(subset_ds, [train_size, val_size])\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n\nprint(f\"Train: {len(train_ds)} | Val: {len(val_ds)}\\n\")\n\n# Load pretrained encoders\nckpt = torch.load(\n    '/kaggle/input/module-1-2-models-only-high-sim/trained_encoders_complete_only_high.pth',\n    map_location=device\n)\n\ntext_encoder = TextEncoder(\n    ckpt['concept_terms'],\n    proj_dim=ckpt['proj_dim']\n).to(device)\n\nimage_encoder = ImageEncoder(\n    proj_dim=ckpt['proj_dim']\n).to(device)\n\ntext_encoder.load_state_dict(ckpt['text_encoder_state_dict'])\nimage_encoder.load_state_dict(ckpt['image_encoder_state_dict'])\n\nprint(\"Encoders loaded (frozen)\\n\")\n\n# Train\nprint(\"=\"*80)\nprint(\"TRAINING SPF MODULE 3 (CORRECTED)\")\nprint(\"=\"*80 + \"\\n\")\n\ntrained_spf = train_spf_module(\n    text_encoder, image_encoder, train_loader, val_loader,\n    epochs=10, lr=1e-4, device=device\n)\n\n# Save final\ntorch.save({\n    'model_state_dict': trained_spf.state_dict(),\n    'config': {\n        'hidden_dim': 500,\n        'num_prototypes': 32,\n        'tau1': trained_spf.tau1.item(),\n        'tau2': trained_spf.tau2.item(),\n        'eta': trained_spf.eta.item(),\n        'gamma1': trained_spf.gamma1.item(),\n        'beta1': trained_spf.beta1.item()\n    }\n}, 'spf_final.pth')\n\nprint(\"\\n✅ Training completed!\")","metadata":{"_uuid":"22a566b6-cd4c-4aa7-8062-28f99976e76b","_cell_guid":"70ad0d6c-ed1e-43e3-8532-bb0d58c5edf7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:19:24.308654Z","iopub.execute_input":"2025-11-12T04:19:24.308889Z","iopub.status.idle":"2025-11-12T06:11:12.611298Z","shell.execute_reply.started":"2025-11-12T04:19:24.308866Z","shell.execute_reply":"2025-11-12T06:11:12.610489Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n\nLoading dataset...\nTrain: 27000 | Val: 3000\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Encoders loaded (frozen)\n\n================================================================================\nTRAINING SPF MODULE 3 (CORRECTED)\n================================================================================\n\nTraining SPF: 1,521,005 params | LR: 0.0001 | Epochs: 10\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   0%|          | 0/422 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\nEpoch 1/10: 100%|██████████| 422/422 [10:07<00:00,  1.44s/it, loss=0.5010, align=0.5866, ω1=0.352, ω2=0.297]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10: Loss=1.1310, Align=0.4857\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.9430, Align=0.6202\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 422/422 [10:02<00:00,  1.43s/it, loss=0.6663, align=0.5861, ω1=0.354, ω2=0.287]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10: Loss=0.6815, Align=0.5823\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.7677, Align=0.6571\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 422/422 [10:01<00:00,  1.43s/it, loss=0.6654, align=0.6085, ω1=0.363, ω2=0.269]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/10: Loss=0.5744, Align=0.6104\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.7021, Align=0.6841\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 422/422 [10:01<00:00,  1.43s/it, loss=0.4677, align=0.6195, ω1=0.379, ω2=0.242]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/10: Loss=0.5276, Align=0.6269\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.6598, Align=0.6934\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 422/422 [10:02<00:00,  1.43s/it, loss=0.5621, align=0.6152, ω1=0.383, ω2=0.230]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/10: Loss=0.4926, Align=0.6351\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.6342, Align=0.6995\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 422/422 [10:02<00:00,  1.43s/it, loss=0.2864, align=0.6481, ω1=0.376, ω2=0.248]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/10: Loss=0.4667, Align=0.6399\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.6165, Align=0.7035\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 422/422 [10:02<00:00,  1.43s/it, loss=0.2453, align=0.6790, ω1=0.357, ω2=0.282]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/10: Loss=0.4405, Align=0.6432\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.5991, Align=0.7072\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 422/422 [10:00<00:00,  1.42s/it, loss=0.2702, align=0.6436, ω1=0.381, ω2=0.238]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/10: Loss=0.4327, Align=0.6458\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.5884, Align=0.7090\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 422/422 [10:01<00:00,  1.43s/it, loss=0.4445, align=0.6404, ω1=0.377, ω2=0.239]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/10: Loss=0.4192, Align=0.6467\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.5830, Align=0.7094\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 422/422 [10:04<00:00,  1.43s/it, loss=0.2211, align=0.6627, ω1=0.371, ω2=0.251]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/10: Loss=0.4161, Align=0.6469\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.5809, Align=0.7096\n✅ Best model saved!\n\n\n✅ Training completed!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"hi","metadata":{"_uuid":"88a3c204-f42b-4e7a-9600-089e95049137","_cell_guid":"80ca3fe0-e1cf-4445-8c3c-3a0dcc281f68","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T06:11:12.612478Z","iopub.execute_input":"2025-11-12T06:11:12.612742Z","iopub.status.idle":"2025-11-12T06:11:12.835950Z","shell.execute_reply.started":"2025-11-12T06:11:12.612717Z","shell.execute_reply":"2025-11-12T06:11:12.834804Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_137/2187410482.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'hi' is not defined"],"ename":"NameError","evalue":"name 'hi' is not defined","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"!mkdir module_3_spf","metadata":{"_uuid":"579d4095-fcad-4f74-a663-5731fc65a9cd","_cell_guid":"8a07ded5-8ec0-451b-a22a-f6fee5904ed3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T06:11:32.842033Z","iopub.execute_input":"2025-11-12T06:11:32.842742Z","iopub.status.idle":"2025-11-12T06:11:33.021538Z","shell.execute_reply.started":"2025-11-12T06:11:32.842713Z","shell.execute_reply":"2025-11-12T06:11:33.020482Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!mv spf_final_corrected.pth spf_final.pth","metadata":{"_uuid":"e27694b5-cb79-490d-bb20-07fb2a147594","_cell_guid":"d2f408aa-e397-4477-93de-a27ea8162e94","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T06:11:12.837589Z","iopub.status.idle":"2025-11-12T06:11:12.837943Z","shell.execute_reply.started":"2025-11-12T06:11:12.837772Z","shell.execute_reply":"2025-11-12T06:11:12.837791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp spf_best.pth module_3_spf/\n!cp spf_final.pth module_3_spf/","metadata":{"_uuid":"1d02d31f-807c-41ea-8395-c2e878bd8dad","_cell_guid":"f1c56d49-e8cf-4af3-b7df-c4a3709648d8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T06:11:50.731906Z","iopub.execute_input":"2025-11-12T06:11:50.732702Z","iopub.status.idle":"2025-11-12T06:11:51.104317Z","shell.execute_reply.started":"2025-11-12T06:11:50.732666Z","shell.execute_reply":"2025-11-12T06:11:51.103258Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import os\nimport json\n\n# Step 1: Create the kaggle.json content\nkaggle_token = {\n    \"username\": \"phanichaitanya349\",\n    \"key\": \"91efb90726e30e3dd48368fddb50908a\"\n}\n\n# Step 2: Save it to ~/.kaggle/kaggle.json\nos.makedirs(\"/root/.kaggle\", exist_ok=True)\nwith open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n    json.dump(kaggle_token, f)\n\n# Step 3: Set proper permissions\nos.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n\nprint(\"✅ kaggle.json created and configured.\")","metadata":{"_uuid":"eee96738-a3b4-4ed7-9e27-72a949855575","_cell_guid":"55fd7a6f-c2ef-4f22-b18a-dc26fe8e0cb2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T06:11:57.130586Z","iopub.execute_input":"2025-11-12T06:11:57.130912Z","iopub.status.idle":"2025-11-12T06:11:57.137516Z","shell.execute_reply.started":"2025-11-12T06:11:57.130879Z","shell.execute_reply":"2025-11-12T06:11:57.136795Z"}},"outputs":[{"name":"stdout","text":"✅ kaggle.json created and configured.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!kaggle datasets init -p module_3_spf","metadata":{"_uuid":"5bae637f-3b0b-4507-aa93-fd7cd5784b2c","_cell_guid":"f3e89708-6c01-4c8e-8767-48b6c0758ac0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T06:12:03.085582Z","iopub.execute_input":"2025-11-12T06:12:03.085897Z","iopub.status.idle":"2025-11-12T06:12:03.807060Z","shell.execute_reply.started":"2025-11-12T06:12:03.085873Z","shell.execute_reply":"2025-11-12T06:12:03.806077Z"}},"outputs":[{"name":"stdout","text":"Data package template written to: module_3_spf/dataset-metadata.json\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import json\n\nmetadata_path = \"module_3_spf/dataset-metadata.json\"\n\n# Load existing metadata\nwith open(metadata_path, \"r\") as f:\n    metadata = json.load(f)\n\n# Set your dataset details\nmetadata[\"title\"] = \"Module 3 SPF 69-10e\"\nmetadata[\"id\"] = \"phanichaitanya349/module-3-spf-69-10e\"  # Must be lowercase with hyphens\nmetadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]  # Open license\n\n# Save the modified metadata\nwith open(metadata_path, \"w\") as f:\n    json.dump(metadata, f, indent=4)\n\nprint(\"✅ Metadata updated successfully.\")","metadata":{"_uuid":"e2b85c55-83fc-4705-ac98-22d5bc0d3904","_cell_guid":"91b1bb49-6643-4868-aae0-b811ae9b5269","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T06:12:58.174508Z","iopub.execute_input":"2025-11-12T06:12:58.175334Z","iopub.status.idle":"2025-11-12T06:12:58.181664Z","shell.execute_reply.started":"2025-11-12T06:12:58.175302Z","shell.execute_reply":"2025-11-12T06:12:58.181084Z"}},"outputs":[{"name":"stdout","text":"✅ Metadata updated successfully.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!kaggle datasets create -p module_3_spf --dir-mode zip","metadata":{"_uuid":"6a5d5ddb-f94d-475f-a14d-9dde52347302","_cell_guid":"3bf9bc1a-31f2-4f54-b487-51b9e0fd4544","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T06:13:08.953993Z","iopub.execute_input":"2025-11-12T06:13:08.954535Z","iopub.status.idle":"2025-11-12T06:13:12.892147Z","shell.execute_reply.started":"2025-11-12T06:13:08.954512Z","shell.execute_reply":"2025-11-12T06:13:12.891201Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file spf_best.pth\n100%|██████████████████████████████████████| 17.4M/17.4M [00:00<00:00, 29.2MB/s]\nUpload successful: spf_best.pth (17MB)\nStarting upload for file spf_final.pth\n100%|██████████████████████████████████████| 5.81M/5.81M [00:00<00:00, 12.4MB/s]\nUpload successful: spf_final.pth (6MB)\nYour private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/phanichaitanya349/module-3-spf-69-10e\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"_uuid":"518c5093-1073-47a5-b43d-39af6c079e1d","_cell_guid":"90ade402-c643-4c00-adbe-a277a190ecfd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}