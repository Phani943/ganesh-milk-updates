{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5787457,"sourceType":"datasetVersion","datasetId":3324119},{"sourceId":12527753,"sourceType":"datasetVersion","datasetId":7908239}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Subset, Dataset, DataLoader, random_split\nfrom transformers import BertTokenizer, BertModel, ViTModel, ViTImageProcessor","metadata":{"_uuid":"64aab576-1fad-424b-a239-b44dd2d5f837","_cell_guid":"4605b5b5-a7f0-4ff4-91fc-929caae95a5c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:26:27.835911Z","iopub.execute_input":"2025-11-12T04:26:27.836236Z","iopub.status.idle":"2025-11-12T04:26:36.199084Z","shell.execute_reply.started":"2025-11-12T04:26:27.836209Z","shell.execute_reply":"2025-11-12T04:26:36.198485Z"}},"outputs":[{"name":"stderr","text":"2025-11-12 04:26:33.259857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762921593.282388     280 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762921593.289369     280 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =============================================================================\n# Dataset\n# =============================================================================\nclass FashionGenDataset(Dataset):\n    def __init__(self, h5_path, visualize=False):\n        self.h5 = h5py.File(h5_path, 'r')\n        self.texts = self.h5['input_description']\n        self.imgs = self.h5['input_image']\n        self.visualize = visualize\n        self.fe = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        caption = self.texts[idx][0].decode('utf-8', errors='ignore')\n        raw_img = Image.fromarray(self.imgs[idx])\n        cropped_img = self.crop_and_resize_img(raw_img)\n        pixel_values = self.fe(images=cropped_img, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n        return caption, pixel_values\n\n    @staticmethod\n    def crop_and_resize_img(img, threshold=245):\n        img_np = np.array(img)\n        non_white_mask = np.any(img_np < threshold, axis=2)\n        if not np.any(non_white_mask):\n            return img.resize((224, 224))\n        coords = np.argwhere(non_white_mask)\n        y0, x0 = coords.min(axis=0)\n        y1, x1 = coords.max(axis=0) + 1\n        cropped_img = img.crop((x0, y0, x1, y1))\n        resized_img = cropped_img.resize((224, 224))\n        return resized_img","metadata":{"_uuid":"d1e694ef-7186-455c-b2b9-4a7715c03603","_cell_guid":"2f6d0e29-c379-4061-aa6b-138b60a3e0c8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:26:36.200349Z","iopub.execute_input":"2025-11-12T04:26:36.200946Z","iopub.status.idle":"2025-11-12T04:26:36.207878Z","shell.execute_reply.started":"2025-11-12T04:26:36.200917Z","shell.execute_reply":"2025-11-12T04:26:36.207178Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# Encoders\n# =============================================================================\nclass TextEncoder(nn.Module):\n    def __init__(self, concept_terms, proj_dim=500, sim_threshold=0.7, freeze_bert=False):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.proj = nn.Linear(768, proj_dim)\n        self.threshold = sim_threshold\n        self.concept_terms = concept_terms\n        self.freeze_bert = freeze_bert\n\n        with torch.no_grad():\n            concept_tokens = self.tokenizer(concept_terms, return_tensors=\"pt\", padding=True, truncation=True)\n            concepts_output = self.bert(**concept_tokens).last_hidden_state\n            concepts_mask = concept_tokens[\"attention_mask\"].unsqueeze(-1)\n            concepts_avg = (concepts_output * concepts_mask).sum(dim=1) / concepts_mask.sum(dim=1)\n        self.register_buffer(\"concept_raw_embeds\", concepts_avg)\n\n    def forward(self, texts):\n        device = next(self.parameters()).device\n        tokens = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n        attn_mask = tokens[\"attention_mask\"].unsqueeze(-1)\n\n        if self.freeze_bert:\n            with torch.no_grad():\n                output = self.bert(**tokens).last_hidden_state\n        else:\n            output = self.bert(**tokens).last_hidden_state\n\n        output_norm = F.normalize(output, dim=-1)\n        concept_norm = F.normalize(self.concept_raw_embeds, dim=-1)\n        sim = torch.matmul(output_norm, concept_norm.T)\n\n        enriched = []\n        for b in range(sim.size(0)):\n            enriched_words = []\n            for l in range(sim.size(1)):\n                if attn_mask[b, l] == 0:\n                    continue\n                sims = sim[b, l]\n                valid_mask = sims >= self.threshold\n                if valid_mask.any():\n                    max_idx = sims[valid_mask].argmax()\n                    concept_idx = valid_mask.nonzero(as_tuple=False)[max_idx]\n                    best_concept = self.concept_raw_embeds[concept_idx]\n                    enriched_word = (output[b, l] + best_concept) / 2\n                else:\n                    enriched_word = output[b, l]\n                enriched_words.append(enriched_word)\n            enriched_sentence = torch.stack(enriched_words).mean(dim=0)\n            enriched.append(enriched_sentence)\n\n        enriched = torch.stack(enriched)\n        return self.proj(enriched)\n\n\nclass ImageEncoder(nn.Module):\n    def __init__(self, proj_dim=500, topk=8):\n        super().__init__()\n        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224', output_attentions=True)\n        self.proj = nn.Linear(768, proj_dim)\n        self.topk = topk\n\n    def forward(self, pixel_values):\n        vit_out = self.vit(pixel_values=pixel_values)\n        out = vit_out.last_hidden_state\n        attn = vit_out.attentions[-1]\n        patches = out[:, 1:]\n        scores = attn[:, :, 0, 1:].mean(dim=1)\n        k = min(self.topk, patches.size(1))\n        idx = scores.topk(k, dim=1).indices\n        selected = patches.gather(1, idx.unsqueeze(-1).expand(-1, -1, patches.size(2)))\n        avg = selected.mean(dim=1)\n        return self.proj(avg)","metadata":{"_uuid":"fae2b516-735b-426e-b480-48d4ef3d94b9","_cell_guid":"ab2c4997-e3d0-4fe4-961c-6844648cf8c7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:26:36.208547Z","iopub.execute_input":"2025-11-12T04:26:36.208747Z","iopub.status.idle":"2025-11-12T04:26:36.222452Z","shell.execute_reply.started":"2025-11-12T04:26:36.208732Z","shell.execute_reply":"2025-11-12T04:26:36.221843Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# =============================================================================\n# SPF Module (Corrected)\n# =============================================================================\nclass SemanticProgressiveFusionModule(nn.Module):\n    def __init__(self, hidden_dim=500, num_prototypes=32, tau1=0.07, tau2=0.1, \n                 eta=1.0, gamma1=0.5, beta1=0.5):\n        super().__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.num_prototypes = num_prototypes\n        \n        self.tau1 = nn.Parameter(torch.tensor(tau1))\n        self.tau2 = nn.Parameter(torch.tensor(tau2))\n        self.eta = nn.Parameter(torch.tensor(eta))\n        self.gamma1 = nn.Parameter(torch.tensor(gamma1))\n        self.beta1 = nn.Parameter(torch.tensor(beta1))\n        \n        # Stage 1\n        self.W_text_1 = nn.Linear(hidden_dim, hidden_dim)\n        self.W_image_1 = nn.Linear(hidden_dim, hidden_dim)\n        \n        # Stage 2\n        self.prototypes = nn.Parameter(torch.randn(num_prototypes, hidden_dim))\n        nn.init.xavier_uniform_(self.prototypes)\n        \n        self.W_image_2 = nn.Linear(hidden_dim, hidden_dim)\n        self.W_text_2 = nn.Linear(hidden_dim, hidden_dim)\n        \n        # Stage 3\n        self.text_out_proj = nn.Linear(hidden_dim, hidden_dim)\n        self.image_out_proj = nn.Linear(hidden_dim, hidden_dim)\n        \n        self.norm_text = nn.LayerNorm(hidden_dim)\n        self.norm_image = nn.LayerNorm(hidden_dim)\n        self.dropout = nn.Dropout(0.1)\n        \n    def compute_entropy(self, attention_weights):\n        eps = 1e-8\n        attention_weights = attention_weights.clamp(min=eps)\n        entropy = -torch.sum(attention_weights * torch.log(attention_weights + eps), dim=-1)\n        return entropy\n    \n    def stage1_fine_grained_alignment(self, X_text, X_image):\n        text_proj = self.W_text_1(X_text)\n        image_proj = self.W_image_1(X_image)\n        \n        S_1 = torch.tanh(torch.matmul(text_proj.unsqueeze(1), image_proj.unsqueeze(2)).squeeze())\n        A_1 = F.softmax(S_1 / self.tau1, dim=0)\n        X_text_1 = X_text + self.gamma1 * A_1.unsqueeze(-1) * X_image\n        entropy_1 = self.compute_entropy(A_1.unsqueeze(-1))\n        \n        return X_text_1, entropy_1, A_1\n    \n    def stage2_part_level_prototypes(self, X_text_1, X_image, A_1):\n        P_2_norm = F.normalize(self.prototypes, dim=-1)\n        image_proj = self.W_image_2(X_image)\n        image_proj_norm = F.normalize(image_proj, dim=-1)\n        \n        proto_sim = torch.matmul(image_proj_norm, P_2_norm.t())\n        R_2 = F.softmax(proto_sim / self.tau2, dim=-1)\n        U_2 = torch.matmul(R_2, self.prototypes)\n        \n        text_proj = self.W_text_2(X_text_1)\n        part_affinity = torch.matmul(text_proj.unsqueeze(1), U_2.unsqueeze(2)).squeeze()\n        \n        # ✅ CRITICAL: beta1 residual\n        A_2 = torch.sigmoid(part_affinity) + self.beta1 * A_1\n        X_text_2 = A_2.unsqueeze(-1) * U_2\n        entropy_2 = self.compute_entropy(R_2)\n        \n        return X_text_2, X_image, entropy_2, R_2\n    \n    def stage3_entropy_regulated_fusion(self, X_text, X_image, \n                                       X_text_1, entropy_1,\n                                       X_text_2, X_image_2, entropy_2):\n        entropy_0 = torch.zeros_like(entropy_1)\n        \n        conf_0 = torch.exp(-entropy_0 / self.eta)\n        conf_1 = torch.exp(-entropy_1 / self.eta)\n        conf_2 = torch.exp(-entropy_2 / self.eta)\n        \n        total_conf = conf_0 + conf_1 + conf_2 + 1e-8\n        \n        omega_0 = conf_0 / total_conf\n        omega_1 = conf_1 / total_conf\n        omega_2 = conf_2 / total_conf\n        \n        X_text_fused = (omega_0.unsqueeze(-1) * X_text + \n                        omega_1.unsqueeze(-1) * X_text_1 + \n                        omega_2.unsqueeze(-1) * X_text_2)\n        \n        X_image_fused = (omega_0.unsqueeze(-1) * X_image + \n                         omega_1.unsqueeze(-1) * X_image + \n                         omega_2.unsqueeze(-1) * X_image_2)\n        \n        X_text_final = self.text_out_proj(X_text_fused)\n        X_image_final = self.image_out_proj(X_image_fused)\n        \n        X_text_final = self.norm_text(X_text_final)\n        X_image_final = self.norm_image(X_image_final)\n        \n        X_text_final = self.dropout(X_text_final)\n        X_image_final = self.dropout(X_image_final)\n        \n        stats = {\n            'omega_0': omega_0.mean().item(),\n            'omega_1': omega_1.mean().item(),\n            'omega_2': omega_2.mean().item(),\n            'entropy_1': entropy_1.mean().item(),\n            'entropy_2': entropy_2.mean().item()\n        }\n        \n        return X_text_final, X_image_final, stats\n    \n    def forward(self, X_text, X_image):\n        X_text_1, entropy_1, A_1 = self.stage1_fine_grained_alignment(X_text, X_image)\n        X_text_2, X_image_2, entropy_2, R_2 = self.stage2_part_level_prototypes(\n            X_text_1, X_image, A_1\n        )\n        X_text_final, X_image_final, stats = self.stage3_entropy_regulated_fusion(\n            X_text, X_image, X_text_1, entropy_1, X_text_2, X_image_2, entropy_2\n        )\n        \n        return X_text_final, X_image_final, stats","metadata":{"_uuid":"bfb4dad5-1c01-423c-8e5e-345ef5bd34db","_cell_guid":"9ad14c92-8962-4abf-97c3-e14dde4f8696","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:26:36.223932Z","iopub.execute_input":"2025-11-12T04:26:36.224109Z","iopub.status.idle":"2025-11-12T04:26:36.245412Z","shell.execute_reply.started":"2025-11-12T04:26:36.224095Z","shell.execute_reply":"2025-11-12T04:26:36.244727Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# =============================================================================\n# Training Functions\n# =============================================================================\ndef train_spf_module(text_encoder, image_encoder, train_loader, val_loader, \n                     epochs=10, lr=1e-4, device='cuda'):\n    spf = SemanticProgressiveFusionModule(\n        hidden_dim=500, num_prototypes=32, tau1=0.07, tau2=0.1,\n        eta=1.0, gamma1=0.5, beta1=0.5\n    ).to(device)\n    \n    optimizer = torch.optim.AdamW(spf.parameters(), lr=lr, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    text_encoder.eval()\n    image_encoder.eval()\n    spf.train()\n    \n    print(f\"Training SPF: {sum(p.numel() for p in spf.parameters()):,} params | LR: {lr} | Epochs: {epochs}\\n\")\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(epochs):\n        epoch_loss = 0.0\n        epoch_align = 0.0\n        num_batches = 0\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for captions, images in progress_bar:\n            images = images.to(device)\n            batch_size = images.size(0)\n            \n            with torch.no_grad():\n                text_feats = text_encoder(captions)\n                image_feats = image_encoder(images)\n            \n            refined_text, refined_image, stats = spf(text_feats, image_feats)\n            \n            # Contrastive loss\n            text_norm = F.normalize(refined_text, dim=-1)\n            image_norm = F.normalize(refined_image, dim=-1)\n            logits = torch.matmul(text_norm, image_norm.t()) / 0.07\n            labels = torch.arange(batch_size, device=device)\n            \n            loss_t2i = F.cross_entropy(logits, labels)\n            loss_i2t = F.cross_entropy(logits.t(), labels)\n            contrastive_loss = (loss_t2i + loss_i2t) / 2\n            \n            # Alignment loss\n            positive_sim = F.cosine_similarity(refined_text, refined_image, dim=1)\n            alignment_loss = -positive_sim.mean()\n            \n            # Entropy regularization\n            entropy_loss = stats['entropy_1'] + stats['entropy_2']\n            \n            # Diversity regularization\n            proto_sim = torch.matmul(\n                F.normalize(spf.prototypes, dim=-1),\n                F.normalize(spf.prototypes, dim=-1).t()\n            )\n            proto_sim = proto_sim - torch.eye(spf.num_prototypes, device=device)\n            diversity_loss = torch.abs(proto_sim).mean()\n            \n            total_loss = (\n                1.0 * contrastive_loss +\n                0.3 * alignment_loss +\n                0.1 * entropy_loss +\n                0.1 * diversity_loss\n            )\n            \n            optimizer.zero_grad()\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(spf.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            epoch_loss += total_loss.item()\n            epoch_align += positive_sim.mean().item()\n            num_batches += 1\n            \n            progress_bar.set_postfix({\n                'loss': f\"{total_loss.item():.4f}\",\n                'align': f\"{positive_sim.mean().item():.4f}\",\n                'ω1': f\"{stats['omega_1']:.3f}\",\n                'ω2': f\"{stats['omega_2']:.3f}\"\n            })\n        \n        avg_loss = epoch_loss / num_batches\n        avg_align = epoch_align / num_batches\n        \n        print(f\"\\nEpoch {epoch+1}/{epochs}: Loss={avg_loss:.4f}, Align={avg_align:.4f}\")\n        \n        scheduler.step()\n        \n        if val_loader is not None:\n            val_loss, val_align = validate_spf(spf, text_encoder, image_encoder, val_loader, device)\n            print(f\"Val: Loss={val_loss:.4f}, Align={val_align:.4f}\")\n            \n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save({\n                    'epoch': epoch + 1,\n                    'model_state_dict': spf.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'val_loss': val_loss,\n                    'config': {\n                        'hidden_dim': 500,\n                        'num_prototypes': 32,\n                        'tau1': spf.tau1.item(),\n                        'tau2': spf.tau2.item(),\n                        'eta': spf.eta.item(),\n                        'gamma1': spf.gamma1.item(),\n                        'beta1': spf.beta1.item()\n                    }\n                }, 'spf_best.pth')\n                print(\"✅ Best model saved!\")\n        print()\n    \n    return spf\n\ndef validate_spf(spf, text_encoder, image_encoder, val_loader, device):\n    spf.eval()\n    total_loss = 0.0\n    total_align = 0.0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for captions, images in val_loader:\n            images = images.to(device)\n            batch_size = images.size(0)\n            \n            text_feats = text_encoder(captions)\n            image_feats = image_encoder(images)\n            refined_text, refined_image, _ = spf(text_feats, image_feats)\n            \n            text_norm = F.normalize(refined_text, dim=-1)\n            image_norm = F.normalize(refined_image, dim=-1)\n            logits = torch.matmul(text_norm, image_norm.t()) / 0.07\n            labels = torch.arange(batch_size, device=device)\n            \n            loss_t2i = F.cross_entropy(logits, labels)\n            loss_i2t = F.cross_entropy(logits.t(), labels)\n            loss = (loss_t2i + loss_i2t) / 2\n            \n            alignment = F.cosine_similarity(refined_text, refined_image, dim=1).mean()\n            \n            total_loss += loss.item()\n            total_align += alignment.item()\n            num_batches += 1\n    \n    spf.train()\n    return total_loss / num_batches, total_align / num_batches","metadata":{"_uuid":"836bc3d7-2c62-4c64-8df3-09f980e65daf","_cell_guid":"db8b9d7f-7789-4af4-bd71-cd7971c2ec82","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:26:36.246013Z","iopub.execute_input":"2025-11-12T04:26:36.246251Z","iopub.status.idle":"2025-11-12T04:26:36.265317Z","shell.execute_reply.started":"2025-11-12T04:26:36.246230Z","shell.execute_reply":"2025-11-12T04:26:36.264624Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# =============================================================================\n# Main Training\n# =============================================================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\\n\")\n\nprint(\"Loading dataset...\")\nfull_ds = FashionGenDataset('/kaggle/input/fashiongen-validation/fashiongen_256_256_train.h5')\n\nsubset_indices = list(range(60000, 90000))\nsubset_ds = Subset(full_ds, subset_indices)\n\ntrain_size = int(0.9 * len(subset_ds))\nval_size = len(subset_ds) - train_size\ntrain_ds, val_ds = random_split(subset_ds, [train_size, val_size])\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n\nprint(f\"Train: {len(train_ds)} | Val: {len(val_ds)}\\n\")\n\n# Load pretrained encoders\nckpt = torch.load(\n    '/kaggle/input/module-1-2-models-only-high-sim/trained_encoders_complete_only_high.pth',\n    map_location=device\n)\n\ntext_encoder = TextEncoder(\n    ckpt['concept_terms'],\n    proj_dim=ckpt['proj_dim']\n).to(device)\n\nimage_encoder = ImageEncoder(\n    proj_dim=ckpt['proj_dim']\n).to(device)\n\ntext_encoder.load_state_dict(ckpt['text_encoder_state_dict'])\nimage_encoder.load_state_dict(ckpt['image_encoder_state_dict'])\n\nprint(\"Encoders loaded (frozen)\\n\")\n\n# Train\nprint(\"=\"*80)\nprint(\"TRAINING SPF MODULE 3 (CORRECTED)\")\nprint(\"=\"*80 + \"\\n\")\n\ntrained_spf = train_spf_module(\n    text_encoder, image_encoder, train_loader, val_loader,\n    epochs=15, lr=1e-4, device=device\n)\n\n# Save final\ntorch.save({\n    'model_state_dict': trained_spf.state_dict(),\n    'config': {\n        'hidden_dim': 500,\n        'num_prototypes': 32,\n        'tau1': trained_spf.tau1.item(),\n        'tau2': trained_spf.tau2.item(),\n        'eta': trained_spf.eta.item(),\n        'gamma1': trained_spf.gamma1.item(),\n        'beta1': trained_spf.beta1.item()\n    }\n}, 'spf_final.pth')\n\nprint(\"\\n✅ Training completed!\")","metadata":{"_uuid":"671b6072-0a69-4697-9725-b349023e3e72","_cell_guid":"87acd53b-9a73-4594-8ab2-6b401cca7163","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T04:26:36.266055Z","iopub.execute_input":"2025-11-12T04:26:36.266581Z","iopub.status.idle":"2025-11-12T07:15:08.811227Z","shell.execute_reply.started":"2025-11-12T04:26:36.266562Z","shell.execute_reply":"2025-11-12T07:15:08.810197Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n\nLoading dataset...\nTrain: 27000 | Val: 3000\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Encoders loaded (frozen)\n\n================================================================================\nTRAINING SPF MODULE 3 (CORRECTED)\n================================================================================\n\nTraining SPF: 1,521,005 params | LR: 0.0001 | Epochs: 15\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/15:   0%|          | 0/422 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\nEpoch 1/15: 100%|██████████| 422/422 [10:11<00:00,  1.45s/it, loss=0.5612, align=0.5683, ω1=0.355, ω2=0.285]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/15: Loss=1.1416, Align=0.4870\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.9281, Align=0.6199\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15: 100%|██████████| 422/422 [10:06<00:00,  1.44s/it, loss=0.4340, align=0.6068, ω1=0.351, ω2=0.297]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/15: Loss=0.6912, Align=0.5754\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.7463, Align=0.6535\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15: 100%|██████████| 422/422 [10:06<00:00,  1.44s/it, loss=0.7859, align=0.5866, ω1=0.347, ω2=0.300]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/15: Loss=0.5563, Align=0.6031\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.6531, Align=0.6768\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15: 100%|██████████| 422/422 [10:06<00:00,  1.44s/it, loss=0.5849, align=0.5975, ω1=0.359, ω2=0.275]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/15: Loss=0.4919, Align=0.6209\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.6072, Align=0.6917\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15: 100%|██████████| 422/422 [10:07<00:00,  1.44s/it, loss=0.2246, align=0.6626, ω1=0.370, ω2=0.256]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/15: Loss=0.4528, Align=0.6306\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.5698, Align=0.6999\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15: 100%|██████████| 422/422 [10:06<00:00,  1.44s/it, loss=0.5041, align=0.6093, ω1=0.368, ω2=0.260]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/15: Loss=0.4139, Align=0.6372\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.5357, Align=0.7065\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15: 100%|██████████| 422/422 [10:05<00:00,  1.44s/it, loss=0.2487, align=0.6490, ω1=0.372, ω2=0.253]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15: Loss=0.3842, Align=0.6452\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.5041, Align=0.7161\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/15: 100%|██████████| 422/422 [10:05<00:00,  1.43s/it, loss=0.2154, align=0.6796, ω1=0.368, ω2=0.264]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/15: Loss=0.3616, Align=0.6518\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.4861, Align=0.7213\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/15: 100%|██████████| 422/422 [10:05<00:00,  1.43s/it, loss=0.2316, align=0.6742, ω1=0.383, ω2=0.234]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/15: Loss=0.3445, Align=0.6573\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.4699, Align=0.7257\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/15: 100%|██████████| 422/422 [10:04<00:00,  1.43s/it, loss=0.2114, align=0.6615, ω1=0.378, ω2=0.240]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/15: Loss=0.3359, Align=0.6610\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.4614, Align=0.7300\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/15: 100%|██████████| 422/422 [10:05<00:00,  1.43s/it, loss=0.2227, align=0.6793, ω1=0.385, ω2=0.230]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/15: Loss=0.3248, Align=0.6642\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.4515, Align=0.7320\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/15: 100%|██████████| 422/422 [10:04<00:00,  1.43s/it, loss=0.1490, align=0.6695, ω1=0.381, ω2=0.238]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/15: Loss=0.3211, Align=0.6664\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.4460, Align=0.7338\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/15: 100%|██████████| 422/422 [10:05<00:00,  1.44s/it, loss=0.2100, align=0.6672, ω1=0.387, ω2=0.226]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/15: Loss=0.3137, Align=0.6677\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.4430, Align=0.7347\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/15: 100%|██████████| 422/422 [10:05<00:00,  1.43s/it, loss=0.2742, align=0.6662, ω1=0.384, ω2=0.228]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/15: Loss=0.3138, Align=0.6684\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.4418, Align=0.7350\n✅ Best model saved!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/15: 100%|██████████| 422/422 [10:03<00:00,  1.43s/it, loss=0.2870, align=0.6688, ω1=0.371, ω2=0.251]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/15: Loss=0.3105, Align=0.6688\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val: Loss=0.4414, Align=0.7351\n✅ Best model saved!\n\n\n✅ Training completed!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!mkdir module_3_spf","metadata":{"_uuid":"0264f179-58bb-4875-9494-6642121dc667","_cell_guid":"60758c57-2279-4112-94f5-6bb54edfa2a7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T07:15:21.746759Z","iopub.execute_input":"2025-11-12T07:15:21.747049Z","iopub.status.idle":"2025-11-12T07:15:21.929015Z","shell.execute_reply.started":"2025-11-12T07:15:21.747028Z","shell.execute_reply":"2025-11-12T07:15:21.928024Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!cp spf_best.pth module_3_spf/\n!cp spf_final.pth module_3_spf/","metadata":{"_uuid":"8f915a01-93ee-4c67-b4a7-14e34897eaa9","_cell_guid":"9e114811-f342-4f92-bbd4-832ac0a8d9aa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T07:15:38.136990Z","iopub.execute_input":"2025-11-12T07:15:38.137823Z","iopub.status.idle":"2025-11-12T07:15:38.528511Z","shell.execute_reply.started":"2025-11-12T07:15:38.137791Z","shell.execute_reply":"2025-11-12T07:15:38.527555Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import os\nimport json\n\n# Step 1: Create the kaggle.json content\nkaggle_token = {\n    \"username\": \"phanichaitanya349\",\n    \"key\": \"91efb90726e30e3dd48368fddb50908a\"\n}\n\n# Step 2: Save it to ~/.kaggle/kaggle.json\nos.makedirs(\"/root/.kaggle\", exist_ok=True)\nwith open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n    json.dump(kaggle_token, f)\n\n# Step 3: Set proper permissions\nos.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n\nprint(\"✅ kaggle.json created and configured.\")","metadata":{"_uuid":"ddaf2b4f-9c6b-44d0-946d-e094b888b467","_cell_guid":"77e7700b-34b3-483a-98d2-4bacd01a6a93","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T07:15:38.530515Z","iopub.execute_input":"2025-11-12T07:15:38.530775Z","iopub.status.idle":"2025-11-12T07:15:38.538698Z","shell.execute_reply.started":"2025-11-12T07:15:38.530750Z","shell.execute_reply":"2025-11-12T07:15:38.538008Z"}},"outputs":[{"name":"stdout","text":"✅ kaggle.json created and configured.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!kaggle datasets init -p module_3_spf","metadata":{"_uuid":"e1137288-e071-4ae4-805d-d788c46df81c","_cell_guid":"a085d24c-f92f-483c-a7be-0ac2d89a873b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T07:15:38.857334Z","iopub.execute_input":"2025-11-12T07:15:38.858026Z","iopub.status.idle":"2025-11-12T07:15:39.689911Z","shell.execute_reply.started":"2025-11-12T07:15:38.858002Z","shell.execute_reply":"2025-11-12T07:15:39.689164Z"}},"outputs":[{"name":"stdout","text":"Data package template written to: module_3_spf/dataset-metadata.json\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import json\n\nmetadata_path = \"module_3_spf/dataset-metadata.json\"\n\n# Load existing metadata\nwith open(metadata_path, \"r\") as f:\n    metadata = json.load(f)\n\n# Set your dataset details\nmetadata[\"title\"] = \"Module 3 SPF 69-15e\"\nmetadata[\"id\"] = \"phanichaitanya349/module-3-spf-69-15e\"  # Must be lowercase with hyphens\nmetadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]  # Open license\n\n# Save the modified metadata\nwith open(metadata_path, \"w\") as f:\n    json.dump(metadata, f, indent=4)\n\nprint(\"✅ Metadata updated successfully.\")","metadata":{"_uuid":"ee9352c6-cfe8-4297-9370-7da0d0e43329","_cell_guid":"a27c9447-5fce-4a65-91c3-ff95c849fc6e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T07:15:39.691181Z","iopub.execute_input":"2025-11-12T07:15:39.691388Z","iopub.status.idle":"2025-11-12T07:15:39.698200Z","shell.execute_reply.started":"2025-11-12T07:15:39.691367Z","shell.execute_reply":"2025-11-12T07:15:39.697460Z"}},"outputs":[{"name":"stdout","text":"✅ Metadata updated successfully.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!kaggle datasets create -p module_3_spf --dir-mode zip","metadata":{"_uuid":"9966d0dc-9414-421a-8e03-258aa9c6bdb5","_cell_guid":"8b54dd8b-14d7-43b2-9fd0-1d5a5a52f64c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-12T07:15:41.862601Z","iopub.execute_input":"2025-11-12T07:15:41.863292Z","iopub.status.idle":"2025-11-12T07:15:46.126358Z","shell.execute_reply.started":"2025-11-12T07:15:41.863268Z","shell.execute_reply":"2025-11-12T07:15:46.125396Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file spf_final.pth\n100%|██████████████████████████████████████| 5.81M/5.81M [00:00<00:00, 8.36MB/s]\nUpload successful: spf_final.pth (6MB)\nStarting upload for file spf_best.pth\n100%|██████████████████████████████████████| 17.4M/17.4M [00:00<00:00, 22.4MB/s]\nUpload successful: spf_best.pth (17MB)\nYour private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/phanichaitanya349/module-3-spf-69-15e\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"_uuid":"206080da-4ce1-4d1a-a945-374229828e1c","_cell_guid":"1d8c16cb-5767-4287-921a-e41ae07f0840","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}